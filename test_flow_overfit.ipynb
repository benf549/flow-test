{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.flow_matching_loss import FlowMatchingLossConfig, FlowMatchingLoss\n",
    "from utils.dataset import UnclusteredProteinChainDataset, idealize_backbone_coords, BatchData, ModelOutput\n",
    "from utils.model import FlowModel, FlowModelModuleConfig\n",
    "from utils.interpolant import Interpolant, InterpolantConfig\n",
    "import utils.openfold_rigid_utils as ur \n",
    "import utils.utility_functions as uf\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_MODEL_CONFIG = FlowModelModuleConfig(\n",
    "    single_repr_node_embedding_dim = 256,\n",
    "    pair_repr_node_embedding_dim = 128,\n",
    "    node_positional_embedding_dim = 128,\n",
    "    node_timestep_embedding_dim = 128,\n",
    "    edge_repr_embedding_dim = 64,\n",
    "    edge_num_distrogram_bins = 22,\n",
    "    edge_embed_diffuse_mask = True,\n",
    "    ipa_hidden_dim = 256,\n",
    "    ipa_no_heads = 8,\n",
    "    ipa_no_qk_points = 8,\n",
    "    ipa_no_v_points = 12,\n",
    "    num_trunk_blocks = 6,\n",
    "    trunk_transformer_atten_heads = 4,\n",
    "    trunk_transformer_num_layers = 2,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "INTERPOLANT_CONFIG = InterpolantConfig(\n",
    "    min_t = 1e-2,\n",
    "    twisting = False,\n",
    "    rots_corrupt = True,\n",
    "    rots_exp_rate = 10,\n",
    "    trans_corrupt = True,\n",
    "    trans_batch_ot = True,\n",
    "    trans_sample_temp = 1.0,\n",
    "    trans_vpsde_bmin = 0.1,\n",
    "    trans_vpsde_bmax = 20.0,\n",
    ")\n",
    "\n",
    "FLOW_MATCHING_LOSS_CONFIG = FlowMatchingLossConfig(\n",
    "    t_normalize_clip = 0.9,\n",
    "    bb_atom_scale = 0.1,\n",
    "    trans_scale = 0.1,\n",
    "    translation_loss_weight = 2.0,\n",
    "    rotation_loss_weights = 1.0,\n",
    "    aux_loss_weight = 0.0,\n",
    "    aux_loss_use_bb_loss = True,\n",
    "    aux_loss_use_pair_loss = True,\n",
    "    aux_loss_t_pass = 0.5,\n",
    ")\n",
    "\n",
    "def get_dummy_data(dataset):\n",
    "    able_chain_A, _ = dataset[dataset.chain_key_to_index['6w70_1-A-A']]\n",
    "    bb_coords = able_chain_A[('A', 'A')]['backbone_coords']\n",
    "    phi_psi_angles = able_chain_A[('A', 'A')]['phi_psi_angles']\n",
    "\n",
    "    ideal_bb_coords = idealize_backbone_coords(bb_coords, phi_psi_angles)\n",
    "    # TODO: center on ligand.\n",
    "    ca_com = uf.compute_center_of_mass(ideal_bb_coords[:, 1])\n",
    "    centered_ideal_bb_coords = uf.center_coords(ideal_bb_coords, ca_com)\n",
    "    frames = uf.compute_residue_frames(centered_ideal_bb_coords)\n",
    "\n",
    "    return frames, centered_ideal_bb_coords, ca_com\n",
    "\n",
    "\n",
    "def get_dummy_batch(dataset) -> BatchData:\n",
    "    target_frames, target_bb_coords, target_ca_com = get_dummy_data(dataset)\n",
    "    target_frames_rigid = ur.Rigid(ur.Rotation(rot_mats=target_frames.unsqueeze(0).expand(10, -1, -1, -1)), target_bb_coords[:, 1].unsqueeze(0).expand(10, -1, -1))\n",
    "\n",
    "    # Create dummy masks for now.\n",
    "    batch_size, num_residues = target_frames_rigid.get_trans().shape[:2]\n",
    "    node_mask = torch.ones(batch_size, num_residues, dtype=torch.float)\n",
    "    diffuse_mask = node_mask.clone()\n",
    "\n",
    "    res_indices = torch.arange(num_residues)[None, ...]\n",
    "\n",
    "    return BatchData(\n",
    "        r_1 = target_frames_rigid, res_mask = node_mask, diffuse_mask = diffuse_mask,\n",
    "        batch_size = batch_size, num_res = num_residues, device = target_frames_rigid.device,\n",
    "        res_indices = res_indices\n",
    "    )\n",
    "\n",
    "def train_epoch(flow_model, interpolant, flow_matching_loss, optimizer, dataset, use_self_conditioning, device):\n",
    "    flow_model.train()\n",
    "    batch = get_dummy_batch(dataset)\n",
    "    batch.to(device)\n",
    "    interpolant.set_device(batch.device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    corrupt_batch = interpolant.corrupt_batch(batch)\n",
    "\n",
    "    # Implements self-conditioning by predicting the transformation once \n",
    "    #   and using it as input to the edge features in the prediction penalized in the loss.\n",
    "    self_conditioning_trans = None\n",
    "    if use_self_conditioning and (torch.rand((1,)).item() > 0.5):\n",
    "        with torch.no_grad():\n",
    "            selfcond_output = flow_model(corrupt_batch)\n",
    "            self_conditioning_trans = (\n",
    "                selfcond_output.pred_trans * corrupt_batch.diffuse_mask[..., None]\n",
    "                + corrupt_batch.trans_1 * (1 - corrupt_batch.diffuse_mask[..., None])\n",
    "            )\n",
    "            corrupt_batch._set_self_condition(self_conditioning_trans)\n",
    "\n",
    "    model_out = flow_model(corrupt_batch)\n",
    "\n",
    "    batch_losses = flow_matching_loss.compute_flow_matching_loss(corrupt_batch, model_out, reduce='mean')\n",
    "    batch_losses.se3_vf_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return batch_losses\n",
    "\n",
    "\n",
    "def sample_model(flow_model, interpolant):\n",
    "    flow_model.eval()\n",
    "    sample_trajectory, clean_sample_trajectory, clean_traj = interpolant.sample(flow_model, 1, 100, 100, self_condition=False)\n",
    "    return sample_trajectory, clean_sample_trajectory, clean_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamically refreshing matplotlib line plot for loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "\n",
    "def plot_loss(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Flow Matching Loss')\n",
    "    plt.ylim(0, 5)\n",
    "    plt.show()\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "file_path = Path('.').parent\n",
    "params = {\n",
    "    'dataset_shelve_path': file_path / 'dataset' / 'dataset_shelve',\n",
    "    'metadata_shelve_path': file_path / 'dataset' / 'metadata_shelve',\n",
    "}\n",
    "\n",
    "learning_rate = 1e-4\n",
    "use_self_conditioning = False\n",
    "\n",
    "flow_model = FlowModel(FLOW_MODEL_CONFIG).to(device)\n",
    "interpolant = Interpolant(INTERPOLANT_CONFIG)\n",
    "flow_matching_loss = FlowMatchingLoss(FLOW_MATCHING_LOSS_CONFIG)\n",
    "dataset = UnclusteredProteinChainDataset(params['dataset_shelve_path'], params['metadata_shelve_path'])\n",
    "optimizer = torch.optim.Adam(flow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Get dummy data target frames.\n",
    "losses = []\n",
    "for epoch in range(1, 250):\n",
    "    batch_losses = train_epoch(flow_model, interpolant, flow_matching_loss, optimizer, dataset, use_self_conditioning, device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for k,v in vars(batch_losses).items():\n",
    "        print(f'{k}: {v.item()}')\n",
    "    print()\n",
    "\n",
    "    losses.append(batch_losses.se3_vf_loss.item())\n",
    "    plot_loss(losses)\n",
    "\n",
    "torch.save(flow_model.state_dict(), 'flow_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamically refreshing matplotlib line plot for loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import IPython.display as display\n",
    "\n",
    "def plot_trajectory(points):\n",
    "    for t, t_data in enumerate(points):\n",
    "        print(t+1)\n",
    "        points = t_data[0, :, 1].detach().cpu().numpy()\n",
    "        px.scatter_3d(x=points[:, 0], y=points[:, 1], z=points[:, 2]).show()\n",
    "        # plot.update_layout(\n",
    "        #     width=1000, \n",
    "        #     height=800,\n",
    "        #     scene=dict(xaxis=dict(range=[-20, 20]), yaxis=dict(range=[-20, 20]), zaxis=dict(range=[-20, 20])))\n",
    "        # plot.show()\n",
    "        time.sleep(0.5)\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_, _, clean_traj = sample_model(flow_model, interpolant)\n",
    "plot_trajectory(clean_traj[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
